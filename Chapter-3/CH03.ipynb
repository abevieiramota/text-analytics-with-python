{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Text Tokenization\n",
    "\n",
    "\"the process of breaking down or splitting textual data into smaller meaningful components called tokens\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Sentence Tokenization/Segmentation\n",
    "\n",
    "básico: splitar em separadores de sentença(., \\n, ; etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "# from pprint import pprint\n",
    "def pprint(l):\n",
    "    \n",
    "    for i in l:\n",
    "        print('\\t> ', i, ' <')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alice = gutenberg.raw(fileids=\"carroll-alice.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_text = \"We will discuss briefly about the basic syntax, structure and design philosophies. There is a defined hierarchical syntax for Python code which you should remember when writing code! Python is a really powerful programming language!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144395"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was\n"
     ]
    }
   ],
   "source": [
    "print(alice[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_st = nltk.sent_tokenize\n",
    "alice_sentences = default_st(text=alice)\n",
    "sample_sentences = default_st(text=sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in sample text: 3\n",
      "Sample text sentences:\n",
      "\t>  We will discuss briefly about the basic syntax, structure and design philosophies.  <\n",
      "\t>  There is a defined hierarchical syntax for Python code which you should remember when writing code!  <\n",
      "\t>  Python is a really powerful programming language!  <\n",
      "\n",
      "Total sentences in alice: 1625\n",
      "First 5 sentences in alice:\n",
      "\t>  [Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I.  <\n",
      "\t>  Down the Rabbit-Hole\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to do: once or twice she had peeped into the\n",
      "book her sister was reading, but it had no pictures or conversations in\n",
      "it, 'and what is the use of a book,' thought Alice 'without pictures or\n",
      "conversation?'  <\n",
      "\t>  So she was considering in her own mind (as well as she could, for the\n",
      "hot day made her feel very sleepy and stupid), whether the pleasure\n",
      "of making a daisy-chain would be worth the trouble of getting up and\n",
      "picking the daisies, when suddenly a White Rabbit with pink eyes ran\n",
      "close by her.  <\n",
      "\t>  There was nothing so VERY remarkable in that; nor did Alice think it so\n",
      "VERY much out of the way to hear the Rabbit say to itself, 'Oh dear!  <\n",
      "\t>  Oh dear!  <\n"
     ]
    }
   ],
   "source": [
    "print(\"Total sentences in sample text:\", len(sample_sentences))\n",
    "print(\"Sample text sentences:\")\n",
    "pprint(sample_sentences)\n",
    "print(\"\\nTotal sentences in alice:\", len(alice_sentences))\n",
    "print(\"First 5 sentences in alice:\")\n",
    "pprint(alice_sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import europarl_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157171\n",
      " \n",
      "Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sit\n"
     ]
    }
   ],
   "source": [
    "german_text = europarl_raw.german.raw(fileids='ep-00-01-17.de')\n",
    "print(len(german_text))\n",
    "print(german_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.tokenize.punkt.PunktSentenceTokenizer'>\n"
     ]
    }
   ],
   "source": [
    "german_sentences_def = default_st(text=german_text, language='german')\n",
    "\n",
    "german_tokenizer = nltk.data.load(resource_url='tokenizers/punkt/german.pickle')\n",
    "german_sentences = german_tokenizer.tokenize(german_text)\n",
    "\n",
    "print(type(german_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(german_sentences_def == german_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>   \n",
      "Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .  <\n",
      "\t>  Wie Sie feststellen konnten , ist der gefürchtete \" Millenium-Bug \" nicht eingetreten .  <\n",
      "\t>  Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden .  <\n",
      "\t>  Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen .  <\n",
      "\t>  Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen - , allen Opfern der Stürme , insbesondere in den verschiedenen Ländern der Europäischen Union , in einer Schweigeminute zu gedenken .  <\n"
     ]
    }
   ],
   "source": [
    "pprint(german_sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "portuguese_text = \"\"\"Na conversa que teve na noite da última segunda-feira (26) com o presidente Michel Temer, o ministro Raul Jungmann pediu \"carta branca\" para fazer as mudanças que julgasse necessárias nos cargos vinculados ao novo Ministério da Segurança Pública. Temer, então, deu a \"carta branca\".\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>  Na conversa que teve na noite da última segunda-feira (26) com o presidente Michel Temer, o ministro Raul Jungmann pediu \"carta branca\" para fazer as mudanças que julgasse necessárias nos cargos vinculados ao novo Ministério da Segurança Pública.  <\n",
      "\t>  Temer, então, deu a \"carta branca\".  <\n"
     ]
    }
   ],
   "source": [
    "pprint(default_st(text=portuguese_text, language='portuguese'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>  We will discuss briefly about the basic syntax, structure and design philosophies.  <\n",
      "\t>  There is a defined hierarchical syntax for Python code which you should remember when writing code!  <\n",
      "\t>  Python is a really powerful programming language!  <\n"
     ]
    }
   ],
   "source": [
    "SENTENCE_TOKENS_PATTERN = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
    "\n",
    "# gaps -> o regex indica os gaps entre os tokens, ou os tokens?\n",
    "regex_st = nltk.tokenize.RegexpTokenizer(pattern=SENTENCE_TOKENS_PATTERN, gaps=True)\n",
    "\n",
    "pprint(regex_st.tokenize(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>  Na conversa que teve na noite da última segunda-feira (26) com o presidente Michel Temer, o ministro Raul Jungmann pediu \"carta branca\" para fazer as mudanças que julgasse necessárias nos cargos vinculados ao novo Ministério da Segurança Pública.  <\n",
      "\t>  Temer, então, deu a \"carta branca\".  <\n"
     ]
    }
   ],
   "source": [
    "pprint(regex_st.tokenize(portuguese_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Tokenization\n",
    "\n",
    "\"the process of splitting or segmenting sentences into their constituent words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The brown fox wasn't that quick and he couldn't win the race.\"\n",
    "\n",
    "default_wt = nltk.word_tokenize\n",
    "words = default_wt(sentence)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'wasn', 't', 'that', 'quick', 'and', 'he', 'couldn', 't', 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "TOKEN_PATTERN = r'\\w+'\n",
    "\n",
    "regex_wt_token = nltk.RegexpTokenizer(pattern=TOKEN_PATTERN, gaps=False)\n",
    "\n",
    "words_token = regex_wt_token.tokenize(sentence)\n",
    "print(words_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'race.']\n"
     ]
    }
   ],
   "source": [
    "GAP_PATTERN = r'\\s+'\n",
    "\n",
    "regex_wt_gaps = nltk.RegexpTokenizer(pattern=GAP_PATTERN, gaps=True)\n",
    "\n",
    "words_gaps = regex_wt_gaps.tokenize(sentence)\n",
    "print(words_gaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The¬brown¬fox¬wasn¬t¬that¬quick¬and¬he¬couldn¬t¬win¬the¬race',\n",
       " \"The¬brown¬fox¬wasn't¬that¬quick¬and¬he¬couldn't¬win¬the¬race.\")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'¬'.join(words_token), '¬'.join(words_gaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 3), (4, 9), (10, 13), (14, 18), (19, 20), (21, 25), (26, 31), (32, 35), (36, 38), (39, 45), (46, 47), (48, 51), (52, 55), (56, 60)]\n",
      "['The', 'brown', 'fox', 'wasn', 't', 'that', 'quick', 'and', 'he', 'couldn', 't', 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "word_indices = list(regex_wt_token.span_tokenize(sentence))\n",
    "\n",
    "print(word_indices)\n",
    "print([sentence[start:end] for start, end in word_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'wasn', \"'\", 't', 'that', 'quick', 'and', 'he', 'couldn', \"'\", 't', 'win', 'the', 'race', '.']\n"
     ]
    }
   ],
   "source": [
    "wordpunkt_wt = nltk.WordPunctTokenizer() # r'\\w+|[^\\w\\s]+'\n",
    "\n",
    "words = wordpunkt_wt.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'race.']\n"
     ]
    }
   ],
   "source": [
    "whitespace_wt = nltk.WhitespaceTokenizer()\n",
    "\n",
    "words = whitespace_wt.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Normalization/Cleansing/Wrangling\n",
    "\n",
    "\"series of steps that should be followed to wrangle, clean, and standardize textual data into a form that could be consumed by other NLP and analytics systems and applications as input.\"\n",
    "\n",
    "tokenization, cleaning, case conversion, correcting spellings, removing stopwords, stemming, lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "corpus = [\"The brown fox wasn't that quick and he couldn't win the race\",\n",
    "          \"Hey that's a great deal! I just bought a phone for $199\",\n",
    "          \"@@You'll (learn) a **lot** in the book. Python is an amazing language!@@\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Text\n",
    "\n",
    "removing extraneus and unnecessary tokens and characters, before further operations like tokenization\n",
    "\n",
    "ex: removendo tags html(*clean_html()* or BeautifulSoup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    \n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    \n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>  [['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race']]  <\n",
      "\t>  [['Hey', 'that', \"'s\", 'a', 'great', 'deal', '!'], ['I', 'just', 'bought', 'a', 'phone', 'for', '$', '199']]  <\n",
      "\t>  [['@', '@', 'You', \"'ll\", '(', 'learn', ')', 'a', '**lot**', 'in', 'the', 'book', '.'], ['Python', 'is', 'an', 'amazing', 'language', '!'], ['@', '@']]  <\n"
     ]
    }
   ],
   "source": [
    "token_list = [tokenize_text(text) for text in corpus]\n",
    "\n",
    "pprint(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"@@You'll\",\n",
       " '(learn)',\n",
       " 'a',\n",
       " '**lot**',\n",
       " 'in',\n",
       " 'the',\n",
       " 'book.',\n",
       " 'Python',\n",
       " 'is',\n",
       " 'an',\n",
       " 'amazing',\n",
       " 'language',\n",
       " '@@']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk = nltk.RegexpTokenizer(pattern='[^\\s\\!\\?]+', gaps=False)\n",
    "\n",
    "tk.tokenize(corpus[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Special Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## after tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^_\\`\\{\\|\\}\\~]\n"
     ]
    }
   ],
   "source": [
    "pattern_str = '[{}]'.format(re.escape(string.punctuation))\n",
    "print(pattern_str)\n",
    "pattern = re.compile(pattern_str)\n",
    "\n",
    "def remove_characters_after_tokenization(tokens):\n",
    "    \n",
    "    # filter(None, ...) -> remove elementos vazios, string vazias etc, por avaliarem False\n",
    "    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n",
    "    \n",
    "    return list(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [[1, 2, 3], [4, 5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t The brown fox was nt that quick and he could nt win the race\n",
      "\t Hey that s a great deal I just bought a phone for 199\n",
      "\t You ll learn a lot in the book Python is an amazing language\n"
     ]
    }
   ],
   "source": [
    "filtered_list_1 = [list(filter(None, \n",
    "                          [remove_characters_after_tokenization(tokens) for tokens in sentence_tokens])) \\\n",
    "                          for sentence_tokens in token_list]\n",
    "\n",
    "for sens in filtered_list_1:\n",
    "    print('\\t', ' '.join((' '.join(sen) for sen in sens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## before tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATTERN_KEEP_APOSTROPHES = r'[?|$|&|*|%|@|(|)|~]'\n",
    "pat_keep_apostrophes = re.compile(PATTERN_KEEP_APOSTROPHES)\n",
    "\n",
    "PATTERN_ONLY_ALPHA = r'[^a-zA-Z0-9 ]'\n",
    "pat_only_alpha = re.compile(PATTERN_ONLY_ALPHA)\n",
    "\n",
    "def remove_characters_before_tokenization(sentence, keep_apostrophes=False):\n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    if keep_apostrophes:\n",
    "        \n",
    "        filtered_sentence = pat_keep_apostrophes.sub('', sentence)\n",
    "    else:\n",
    "        \n",
    "        filtered_sentence = pat_only_alpha.sub('', sentence)\n",
    "        \n",
    "    return filtered_sentence\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>  The brown fox wasnt that quick and he couldnt win the race  <\n",
      "\t>  Hey thats a great deal I just bought a phone for 199  <\n",
      "\t>  Youll learn a lot in the book Python is an amazing language  <\n"
     ]
    }
   ],
   "source": [
    "pprint([remove_characters_before_tokenization(sentence) for sentence in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>  The brown fox wasn't that quick and he couldn't win the race  <\n",
      "\t>  Hey that's a great deal! I just bought a phone for 199  <\n",
      "\t>  You'll learn a lot in the book. Python is an amazing language!  <\n"
     ]
    }
   ],
   "source": [
    "cleaned_corpus = [remove_characters_before_tokenization(sentence, keep_apostrophes=True) for sentence in corpus]\n",
    "pprint(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanding Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from contractions import CONTRACTION_MAP\n",
    "\n",
    "def expand_contractions(sentence, contraction_mapping):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), flags=re.IGNORECASE|re.DOTALL)\n",
    "    \n",
    "    def expand_match(contraction):\n",
    "        \n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                               if contraction_mapping.get(match)\\\n",
    "                               else contraction_mapping.get(match.lower())\n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        \n",
    "        return expanded_contraction\n",
    "    \n",
    "    expanded_sentence = contractions_pattern.sub(expand_match, sentence)\n",
    "    \n",
    "    return expanded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>  The brown fox was not that quick and he could not win the race  <\n",
      "\t>  Hey that is a great deal! I just bought a phone for 199  <\n",
      "\t>  You will learn a lot in the book. Python is an amazing language!  <\n"
     ]
    }
   ],
   "source": [
    "expanded_corpus = [expand_contractions(sentence, CONTRACTION_MAP) for sentence in cleaned_corpus]\n",
    "\n",
    "pprint(expanded_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  The brown fox wasn't that quick and he couldn't win the race\n",
      "Lower:  the brown fox wasn't that quick and he couldn't win the race\n",
      "Upper:  THE BROWN FOX WASN'T THAT QUICK AND HE COULDN'T WIN THE RACE\n",
      "Sentence case:  The brown fox wasn't that quick and he couldn't win the race\n",
      "Word case:  The Brown Fox Wasn'T That Quick And He Couldn'T Win The Race\n"
     ]
    }
   ],
   "source": [
    "print('Original: ', corpus[0])\n",
    "print('Lower: ', corpus[0].lower())\n",
    "print('Upper: ', corpus[0].upper())\n",
    "print('Sentence case: ', corpus[0].capitalize())\n",
    "print('Word case: ', corpus[0].title())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Stopwords\n",
    "\n",
    "\"words that have little or no significance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    \n",
    "    stopword_list = nltk.corpus.stopwords.words('english')\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    \n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>  [['The', 'brown', 'fox', 'quick', 'could', 'win', 'race']]  <\n",
      "\t>  [['Hey', 'great', 'deal', '!'], ['I', 'bought', 'phone', '199']]  <\n",
      "\t>  [['You', 'learn', 'lot', 'book', '.'], ['Python', 'amazing', 'language', '!']]  <\n"
     ]
    }
   ],
   "source": [
    "expanded_corpus_tokens = [tokenize_text(text) for text in expanded_corpus]\n",
    "\n",
    "filtered_list_3 = [[remove_stopwords(tokens) for tokens in sentence_tokens] for sentence_tokens in expanded_corpus_tokens]\n",
    "\n",
    "pprint(filtered_list_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!!!! \n",
    "\n",
    "'One important thing to remember is that negations like not and no are removed in this case(...) and it is often essential to preserve the same so the actual context of the sentence is not lost'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correcting Words\n",
    "\n",
    "\"The definition of incorrect here covers words that have spelling mistakes as well as words with several letters repreated that do not contribute much to its overall significance.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting repeated characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1 Word: finalllllyy\n",
      "Step: 2 Word: finallllly\n",
      "Step: 3 Word: finalllly\n",
      "Step: 4 Word: finallly\n",
      "Step: 5 Word: finally\n",
      "Step: 6 Word: finaly\n",
      "Final word:  finaly\n"
     ]
    }
   ],
   "source": [
    "old_word = 'finalllllyyy'\n",
    "\n",
    "repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "match_substitution = r'\\1\\2\\3'\n",
    "\n",
    "step = 1\n",
    "\n",
    "while True:\n",
    "    \n",
    "    new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "    \n",
    "    if new_word != old_word:\n",
    "        \n",
    "        print('Step: {} Word: {}'.format(step, new_word))\n",
    "        step += 1\n",
    "        \n",
    "        old_word = new_word\n",
    "        continue\n",
    "    else:\n",
    "        \n",
    "        print('Final word: ', new_word)\n",
    "        break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abelardo Vieira Mota'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove(match):\n",
    "    \n",
    "    if match.group(1):\n",
    "        return match.group(1)[0]\n",
    "    else: \n",
    "        return match\n",
    "    \n",
    "repeat_pattern = re.compile(r'(\\w)(\\1+)')\n",
    "\n",
    "def remove_repeat(s):\n",
    "    \n",
    "    return repeat_pattern.sub(remove, s)\n",
    "\n",
    "remove_repeat('Abelaaaaardooo Vieirrrra Moooota')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1 Word: finalllllyy\n",
      "Step: 2 Word: finallllly\n",
      "Step: 3 Word: finalllly\n",
      "Step: 4 Word: finallly\n",
      "Step: 5 Word: finally\n",
      "Final correct word:  finally\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "old_word = 'finalllllyyy'\n",
    "\n",
    "repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "match_substitution = r'\\1\\2\\3'\n",
    "\n",
    "step = 1\n",
    "\n",
    "while True:\n",
    "    \n",
    "    if wordnet.synsets(old_word):\n",
    "        print('Final correct word: ', old_word)\n",
    "        break\n",
    "    new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "    \n",
    "    if new_word != old_word:\n",
    "        \n",
    "        print('Step: {} Word: {}'.format(step, new_word))\n",
    "        step += 1\n",
    "        \n",
    "        old_word = new_word\n",
    "        continue\n",
    "    else:\n",
    "        \n",
    "        print('Final word: ', new_word)\n",
    "        break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_repeated_characters(tokens):\n",
    "    \n",
    "    def replace(old_word):\n",
    "        if wordnet.synsets(old_word):\n",
    "            return old_word\n",
    "        \n",
    "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "        return replace(new_word) if new_word != old_word else new_word\n",
    "    \n",
    "    correct_tokens = [replace(word) for word in tokens]\n",
    "    \n",
    "    return correct_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'schooool', 'is', 'reallllllyyyy', 'amaaaaazinnnng']\n",
      "['My', 'school', 'is', 'really', 'amazing']\n"
     ]
    }
   ],
   "source": [
    "sample_sentence = 'My schooool is reallllllyyyy amaaaaazinnnng'\n",
    "sample_sentence_tokens = tokenize_text(sample_sentence)[0]\n",
    "\n",
    "print(sample_sentence_tokens)\n",
    "print(remove_repeated_characters(sample_sentence_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Correcting Spellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>  ('the', 80030)  <\n",
      "\t>  ('of', 40025)  <\n",
      "\t>  ('and', 38313)  <\n",
      "\t>  ('to', 28766)  <\n",
      "\t>  ('in', 22050)  <\n",
      "\t>  ('a', 21155)  <\n",
      "\t>  ('that', 12512)  <\n",
      "\t>  ('he', 12401)  <\n",
      "\t>  ('was', 11410)  <\n",
      "\t>  ('it', 10681)  <\n"
     ]
    }
   ],
   "source": [
    "import re, collections\n",
    "\n",
    "def tokens(text):\n",
    "    \n",
    "    return re.findall('[a-z]+', text.lower())\n",
    "\n",
    "with open('big.txt') as f:\n",
    "    WORDS = tokens(f.read())\n",
    "    \n",
    "WORD_COUNTS = collections.Counter(WORDS)\n",
    "\n",
    "pprint(WORD_COUNTS.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edits0(word):\n",
    "    \n",
    "    return {word}\n",
    "\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "def edits1(word):\n",
    "        \n",
    "    def splits(word):\n",
    "\n",
    "        return [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "\n",
    "    pairs = splits(word)\n",
    "    deletes = [a + b[1:] for (a, b) in pairs if b]\n",
    "    transposes = [a + b[1] + b[0] + b[2:] for (a, b) in pairs if len(b) > 1]\n",
    "    replaces = [a+c+b[1:] for (a, b) in pairs for c in alphabet if b]\n",
    "    inserts = [a+c+b for (a, b) in pairs for c in alphabet]\n",
    "    \n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word):\n",
    "    \n",
    "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def known(words):\n",
    "    \n",
    "    return {w for w in words if w in WORD_COUNTS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fianlly'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'fianlly'\n",
    "\n",
    "edits0(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known(edits0(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'finally'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known(edits1(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'faintly', 'finally', 'finely', 'frankly'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known(edits2(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'finally'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates = (known(edits0(word)) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct(word):\n",
    "    \n",
    "    candidates = (known(edits0(word)) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "    \n",
    "    return max(candidates, key=WORD_COUNTS.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finally'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct('fianlly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FIANLY'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct('FIANLY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct_match(match):\n",
    "    \n",
    "    word = match.group()\n",
    "    \n",
    "    def case_of(text):\n",
    "        \n",
    "        return (str.upper if text.isupper() else\n",
    "                str.lower if text.islower() else\n",
    "                str.title if text.istitle() else\n",
    "                str)\n",
    "    \n",
    "    return case_of(word)(correct(word.lower()))\n",
    "\n",
    "def correct_text_generic(text):\n",
    "    \n",
    "    return re.sub('[a-zA-Z]+', correct_match, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finally'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_text_generic('fianlly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FINALLY'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_text_generic('FIANLY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pattern'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-26f3bd9b8a19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msuggest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pattern'"
     ]
    }
   ],
   "source": [
    "# pattern is only available to python 3.6 with dev branch...\n",
    "from pattern.en import suggest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
