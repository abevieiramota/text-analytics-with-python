{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contractions import CONTRACTION_MAP\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = nltk.corpus.stopwords.words(\"english\")\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), \n",
    "                                    flags=re.IGNORECASE|re.DOTALL)\n",
    "\n",
    "def expand_contractions(text):\n",
    "    \n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match, match.lower())\n",
    "        expanded_contraction = first_char + expanded_contraction[1:]\n",
    "        \n",
    "        return expanded_contraction\n",
    "    \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    \n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def pos_tag_text(text):\n",
    "    \n",
    "    return [(w, get_wordnet_pos(t)) for w, t in nltk.pos_tag(text)]\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \n",
    "    pos_tagged_text = pos_tag_text(text)\n",
    "    lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag else word\\\n",
    "                        for word, pos_tag in pos_tagged_text]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    \n",
    "    tokens = tokenize_text(text)\n",
    "    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    \n",
    "    tokens = tokenize_text(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [expand_contractions, lemmatize_text, remove_special_characters, remove_stopwords]\n",
    "\n",
    "def normalize_corpus(corpus, tokenize=False):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    \n",
    "    for text in corpus:\n",
    "        for step in pipeline:\n",
    "            text = step(text)\n",
    "            \n",
    "        normalized_corpus.append(text)\n",
    "        if tokenize: # estranho\n",
    "            text = tokenize_text(text)\n",
    "            normalized_corpus.append(text)\n",
    "            \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS = [\n",
    "    'the sky is blue',\n",
    "    'sky is blue and sky is beautiful',\n",
    "    'the beautiful sky is so blue',\n",
    "    'i love blue cheese'\n",
    "]\n",
    "\n",
    "new_doc = ['loving this blue sky today']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gf(vector, labels):\n",
    "    \n",
    "    return pd.DataFrame(vector, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  and sky  beautiful  beautiful sky  blue  blue and  blue cheese  \\\n",
      "0    0        0          0              0     1         0            0   \n",
      "1    1        1          1              0     1         1            0   \n",
      "2    0        0          1              1     1         0            0   \n",
      "3    0        0          0              0     1         0            1   \n",
      "\n",
      "   cheese  is  is beautiful   ...     is so  love  love blue  sky  sky is  so  \\\n",
      "0       0   1             0   ...         0     0          0    1       1   0   \n",
      "1       0   2             1   ...         0     0          0    2       2   0   \n",
      "2       0   1             0   ...         1     0          0    1       1   1   \n",
      "3       1   0             0   ...         0     1          1    0       0   0   \n",
      "\n",
      "   so blue  the  the beautiful  the sky  \n",
      "0        0    1              0        1  \n",
      "1        0    0              0        0  \n",
      "2        1    1              1        0  \n",
      "3        0    0              0        0  \n",
      "\n",
      "[4 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bow_vectorizer = CountVectorizer(min_df=1, ngram_range=(1, 2))\n",
    "bow_features = bow_vectorizer.fit_transform(CORPUS)\n",
    "\n",
    "print(gf(bow_features.todense(), bow_vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0\n",
      "and            0\n",
      "and sky        0\n",
      "beautiful      0\n",
      "beautiful sky  0\n",
      "blue           1\n",
      "blue and       0\n",
      "blue cheese    0\n",
      "cheese         0\n",
      "is             0\n",
      "is beautiful   0\n",
      "is blue        0\n",
      "is so          0\n",
      "love           0\n",
      "love blue      0\n",
      "sky            1\n",
      "sky is         0\n",
      "so             0\n",
      "so blue        0\n",
      "the            0\n",
      "the beautiful  0\n",
      "the sky        0\n"
     ]
    }
   ],
   "source": [
    "new_doc_features = bow_vectorizer.transform(new_doc)\n",
    "print(gf(new_doc_features.todense(), bow_vectorizer.get_feature_names()).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "def tfidf_transformer(bow_matrix):\n",
    "    \n",
    "    transformer = TfidfTransformer(norm='l2',\n",
    "                                  smooth_idf=True,\n",
    "                                  use_idf=True)\n",
    "    tfidf_matrix = transformer.fit_transform(bow_matrix)\n",
    "    \n",
    "    return transformer, tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  and sky  beautiful  beautiful sky  blue  blue and  blue cheese  \\\n",
      "0  0.00     0.00       0.00           0.00  0.27      0.00         0.00   \n",
      "1  0.31     0.31       0.24           0.00  0.16      0.31         0.00   \n",
      "2  0.00     0.00       0.28           0.36  0.19      0.00         0.00   \n",
      "3  0.00     0.00       0.00           0.00  0.25      0.00         0.48   \n",
      "\n",
      "   cheese    is  is beautiful   ...     is so  love  love blue   sky  sky is  \\\n",
      "0    0.00  0.33          0.00   ...      0.00  0.00       0.00  0.33    0.33   \n",
      "1    0.00  0.40          0.31   ...      0.00  0.00       0.00  0.40    0.40   \n",
      "2    0.00  0.23          0.00   ...      0.36  0.00       0.00  0.23    0.23   \n",
      "3    0.48  0.00          0.00   ...      0.00  0.48       0.48  0.00    0.00   \n",
      "\n",
      "     so  so blue   the  the beautiful  the sky  \n",
      "0  0.00     0.00  0.41           0.00     0.52  \n",
      "1  0.00     0.00  0.00           0.00     0.00  \n",
      "2  0.36     0.36  0.28           0.36     0.00  \n",
      "3  0.00     0.00  0.00           0.00     0.00  \n",
      "\n",
      "[4 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "\n",
    "tfidf_trans, tfidf_features = tfidf_transformer(bow_features)\n",
    "features = np.round(tfidf_features.todense(), 2)\n",
    "print(gf(features, feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf_extractor(corpus, ngram_range=(1, 1)):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(min_df=1,\n",
    "                                 norm='l2',\n",
    "                                 smooth_idf=True,\n",
    "                                 use_idf=True,\n",
    "                                 ngram_range=ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    return vectorizer, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n",
      "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n",
      "2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n",
      "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer, tfidf_features = tfidf_extractor(CORPUS)\n",
    "\n",
    "print(gf(np.round(tfidf_features.todense(), 2), tfidf_vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love   sky   so  the\n",
      "0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "nd_tfidf = tfidf_vectorizer.transform(new_doc)\n",
    "\n",
    "print(gf(np.round(nd_tfidf.todense(), 2), tfidf_vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Word Vectorization Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "\n",
    "TOKENIZED_CORPUS = [nltk.word_tokenize(sentence) for sentence in CORPUS]\n",
    "tokenized_new_doc = [nltk.word_tokenize(sentence) for sentence in new_doc]\n",
    "\n",
    "model = gensim.models.Word2Vec(TOKENIZED_CORPUS, size=10, window=10, min_count=2, sample=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04033279  0.04343031 -0.01067983  0.00521895  0.00383821  0.03068873\n",
      "  0.00573959  0.04973888  0.02468563 -0.01511299]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(model['sky'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03642776  0.03646963  0.0220179  -0.0389447   0.02501153 -0.03793925\n",
      "  0.04849996  0.00477016 -0.03181459  0.00453657]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(model['blue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,), dtype='float64')\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "            \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    \n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features) for tokenized_sentence in corpus]\n",
    "    \n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.018  0.038 -0.002  0.001 -0.006 -0.01   0.019  0.033  0.019  0.007]\n",
      " [-0.001  0.034 -0.006 -0.004 -0.012  0.002  0.012  0.029  0.024  0.005]\n",
      " [ 0.009  0.033 -0.007 -0.008 -0.014 -0.004  0.01   0.026  0.021  0.006]\n",
      " [ 0.036  0.036  0.022 -0.039  0.025 -0.038  0.048  0.005 -0.032  0.005]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "avg_word_vec_features = averaged_word_vectorizer(corpus=TOKENIZED_CORPUS, model=model, num_features=10)\n",
    "\n",
    "print(np.round(avg_word_vec_features, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.002  0.04   0.006 -0.017  0.014 -0.004  0.027  0.027 -0.004 -0.005]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "nd_avg_word_vec_features = averaged_word_vectorizer(corpus=tokenized_new_doc, model=model, num_features=10)\n",
    "\n",
    "print(np.round(nd_avg_word_vec_features, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "actual_labels = [0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1]\n",
    "predicted_labels = [0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0]\n",
    "\n",
    "ac = Counter(actual_labels)\n",
    "pc = Counter(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual counts: [(0, 10), (1, 10)]\n"
     ]
    }
   ],
   "source": [
    "print('Actual counts:', ac.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted counts: [(0, 11), (1, 9)]\n"
     ]
    }
   ],
   "source": [
    "print('Predicted counts:', pc.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Predicted:    \n",
      "                   spam ham\n",
      "Actual: spam          5   5\n",
      "        ham           6   4\n"
     ]
    }
   ],
   "source": [
    "cm = metrics.confusion_matrix(y_true=actual_labels, y_pred=predicted_labels, labels=[0, 1])\n",
    "\n",
    "print(pd.DataFrame(data=cm, columns=pd.MultiIndex(levels=[['Predicted:'],\n",
    "                                                         ['spam', 'ham']],\n",
    "                                                  labels=[[0, 0], [0, 1]]),\n",
    "                   index=pd.MultiIndex(levels=[['Actual:'],\n",
    "                                              ['spam', 'ham']],\n",
    "                                       labels=[[0, 0], [0, 1]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is defined as the overall accuracy or proportion of correct predictions of the\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision is defined as the number of predictions made that are actually correct\n",
    "or relevant out of all the predictions based on the positive class. This is also known as\n",
    "positive predictive value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall is defined as the number of instances of the positive class that were correctly\n",
    "predicted. This is also known as hit rate, coverage, or sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 score is another accuracy measure that is computed by taking the harmonic mean\n",
    "of the precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Multi-Class Classification System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "def get_data():\n",
    "    \n",
    "    data = fetch_20newsgroups(subset='all',\n",
    "                              shuffle=True,\n",
    "                              remove=('headers', 'footers', 'quotes'))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def prepare_datasets(corpus, labels, test_size=.33):\n",
    "    \n",
    "    train_X, test_X, train_Y, test_Y = train_test_split(corpus, labels, test_size=test_size, random_state=42)\n",
    "    \n",
    "    return train_X, test_X, train_Y, test_Y\n",
    "\n",
    "def remove_empty_docs(corpus, labels):\n",
    "    \n",
    "    filtered_corpus = []\n",
    "    filtered_labels = []\n",
    "    \n",
    "    for doc, label in zip(corpus, labels):\n",
    "        \n",
    "        if doc.strip():\n",
    "            filtered_corpus.append(doc)\n",
    "            filtered_labels.append(label)\n",
    "            \n",
    "    return filtered_corpus, filtered_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "dataset = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, labels = dataset.data, dataset.target\n",
    "corpus, labels = remove_empty_docs(corpus, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample document: the blood of the lamb.\n",
      "\n",
      "This will be a hard task, because most cultures used most animals\n",
      "for blood sacrifices. It has to be something related to our current\n",
      "post-modernism state. Hmm, what about used computers?\n",
      "\n",
      "Cheers,\n",
      "Kent\n",
      "Class label: 19\n",
      "Actual class label: talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "print('Sample document:', corpus[10])\n",
    "print('Class label:', labels[10])\n",
    "print('Actual class label:', dataset.target_names[labels[10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus, test_corpus, train_labels, test_labels = prepare_datasets(corpus, labels, test_size=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuar pag. 206"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
